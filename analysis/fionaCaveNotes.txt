### set up a git repo for this. ###
git remote add origin https://github.com/danchurch/cavePseudogymnoascus.git
git branch -M main
git remote set-url origin git@github.com:danchurch/cavePseudogymnoascus.git
git push -u origin main
###########################################

## see what we can do with the reads from the cave

########## install and play with qiime2 #############

## it is requested that I use qiime2. Great. 

## following:
https://docs.qiime2.org/2021.11/install/native/

## qiime says install in a conda environment. Fine by me, contain the beast. 

wget https://data.qiime2.org/distro/core/qiime2-2021.11-py38-linux-conda.yml

conda env create -n qiime2-2021.11 --file qiime2-2021.11-py38-linux-conda.yml

conda activate qiime2-2021.11 

## great. and to get tab completion, they have a script ready:

source tab-qiime

## not sure if there is some sort of rc for conda environments?
## qiime says put it in bashrc. wait on that. 

#############################

## so, fiona has 16 samples?:

Fiona-1
Fiona-2
Fiona-3
Fiona-4
Fiona-5
Fiona-6
Fiona-7
Fiona-8
FIona-9
Fiona-10
Fiona-11
Fiona-16Euk
Fiona-19Euk
Fiona-20Euk
Fiona-21Euk
Fiona-22Euk

## let's get these and remove the others.
## and go fishing for primers...

## do this in pandas

python3
import os
import pandas as pd

readDir = "/home/daniel/Documents/analyses/FionaCave/otherFastqs"

## our read files are:
fastqs = pd.Series(os.listdir(readDir))

## one useful sample info spreadsheet is here
aa = pd.read_excel('/home/daniel/Documents/analyses/FionaCave/metadata/WeigQCchecks/OeMik-Exp009_Proben.xlsx')
## we just need those with fiona in the "Probe Id" column value, 
## and just the "Probe Id" and "Genomics Id" columns: 
bb = aa[["Genomics Id","Probe Id"]]
mask = bb["Probe Id"].str.contains('Fiona').fillna(False)
cc = bb[mask]
## it looks like this missed one sample, where "Fiona" is spelled with a big "I"
cc = cc.append(bb[bb["Probe Id"].str.contains('FIona-9').fillna(False)])
cc.reset_index(inplace=True, drop=True)

## now, we want to go through our fasuqs series, and pull out the matches to our Genomics Id column...

def getFN(GI):
    mask = fastqs.str.contains(GI)
    fn = fastqs[mask].values[0]
    return(fn)

cc['filename'] = cc['Genomics Id'].apply(getFN)

cc.to_csv('fionaFileMap.csv', index=False)

## and for the shell loop, just the filenames:

cc['filename'].to_csv('fionaFileList', header=False, index=False)

## now out to shell, to save just these files:

#mkdir fionaFastqs
ddir=/home/daniel/Documents/analyses/FionaCave/otherFastqs/
fionaFastqs=/home/daniel/Documents/analyses/FionaCave/fionaFastqs/

for x in $(cat fionaFileList)
do
    mv $ddir$x $fionaFastqs
done

## and back into this for for a moment, we want to the illumina indexes
## make sure they aren't floating around somewhere

readDir = "/home/daniel/Documents/analyses/FionaCave/"

os.path.exists(readDir+'fionaFileMap.csv')

aa = pd.read_csv(readDir+'fionaFileMap.csv', index_col='Genomics Id')

## get sample indexes. There are two...why? these are uni-directional, right?
## keep them for the moment, maybe this will make sense later...

indexCSV = "/home/daniel/Documents/analyses/FionaCave/metadata/tillmannPrimer/weigIndexInfo.csv"
os.path.exists(indexCSV)
indexInfo = pd.read_csv(indexCSV, index_col='Sample_Name')
indexInfo = indexInfo[['index', 'index2']]
indexInfo.head()
bb = aa.join(indexInfo, how='left')
bb.reset_index(inplace=True)
bb.columns = ['sampleName','description', 'fileName','index','index2']
## update our fiona file info csv
bb.to_csv('fionaFileMap.csv',  index=False)
bb = pd.read_csv('fionaFileMap.csv')

fionaInfo = pd.read_csv('fionaFileMap.csv')


## great, now we need metadata....


## attach the pool that a sample is from?

## and can we get the forward and reverse 16s and 18s barcodes 
## for these?

## from Tillmann email:

## Prok_16S (~300 bp)
## ilu_515f
## TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTGYCAGCMGCCGCGGTAA
## ilu_806rN
## GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAGGGACTACNVGGGTWTCTAAT

## Euk_18S (~500 bp)
## ilu_GA20F           
## TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTAACTTCGGGAWAAGGATTGGCT
## ilu_RM9R
## GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAGAGAGTCAARCTCAACAGGGTCTT 



## great, so now, let's get some quality graphics for our read sets, etc.

## we want to import our read files into qiime...

## not sure what the illumina pipelines are these days...
## this may be casava1.8?

## an example data set is available from qiime for comparison:

conda activate qiime2-2021.11

exFastq='/home/daniel/Desktop/casava-18-single-end-demultiplexed/L1S8_8_L001_R1_001.fastq'

head $exFastq

## let's check for primers in there...

f16s=TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTGYCAGCMGCCGCGGTAA
r16s=GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAGGGACTACNVGGGTWTCTAAT
f16sRC=TTACCGCGGCMGCTGYCACCTGTCTCTTATACACATCTGACGCTGCCGACGA
r16sRC=ATTAGAWACCCVNGTAGTCCCTGTCTCTTATACACATCTCCGAGCCCACGAGAC

f18s=TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTAACTTCGGGAWAAGGATTGGCT
r18s=GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAGAGAGTCAARCTCAACAGGGTCTT
f18sRC=AGCCAATCCTTWTCCCGAAGTTACCTGTCTCTTATACACATCTGACGCTGCCGACGA
r18sRC=AAGACCCTGTTGAGRTTGACTCTCTGTCTCTTATACACATCTCCGAGCCCACGAGAC

ffq=/home/daniel/Documents/analyses/FionaCave/fionaFastqUnz/

grep -R GAGTCAGCAGC $ffq 
grep -R GAGTCAGCAGC $ffq | wc -l

grep -R $f16s $ffq
grep -R $r16s $ffq
grep -R $f16sRC $ffq
grep -R $r16sRC $ffq

grep -R $f18s $ffq
grep -R $r18s $ffq
grep -R $f18sRC $ffq
grep -R $r18sRC $ffq

## no exact matches. I assume since this is all demultiplexed,
## he pulled out phyx, etc. 

## how are the read depths? and quality

cut -d , -f 2 fionaFileMap.csv

exFQ=/home/daniel/Documents/analyses/FionaCave/fionaFastaqsUnz/Oemik-000687_S101_L001_R1_001.fastq

fastqc fionaFastqs/Oemik-000687_S101_L001_R1_001.fastq.gz  \

fastqc fionaFastqs/*  \
-o fastQCoutputs \
-- extract

## it would be nice to get a total view of the fiona runs,

cat * > allFionaSamples.fastq

fastqc allFionaSamples.fastq \
-o fastQCoutputs \
--extract

## oh jeez. this is some messy data
## the vast majority of the reads are 
## ~150 BP in length (60,000 or so), less than 20000 
## are in the right read length

## might be useful to break this down by primer
## sets...


## so onward with qiime, to bring this in as an "artifact"

conda activate qiime2-2021.11

qiime tools import \
  --type 'SampleData[SequencesWithQuality]' \
  --input-path /home/daniel/Documents/analyses/FionaCave/fionaFastqs \
  --input-format CasavaOneEightSingleLanePerSampleDirFmt \
  --output-path fionaRawSeqs.qza

## and we need some kind of metadata here
## the only metadata I think we have are what pool a sample came from. 

## side note, for github, how do make sure we're not putting on 
## large files, before sending them to github? with find:

find -size +10M -exec du -h {} \;


## okay, for the moment, let's confirm that the Eukaryotic reads are 
## the source of the problem. back in python

## the 16s reads are:

wd = '/home/daniel/Documents/analyses/FionaCave/'

aa = pd.read_csv(wd+'fionaFileMap.csv')

prokDF = aa[~aa['description'].str.contains('Euk')]

prokDF['fileName'].to_csv('prokFileList', header=False, index=False)


eukDF = aa[aa['description'].str.contains('Euk')]

eukDF['fileName'].to_csv('eukFileList', header=False, index=False)

## now, we should be able to cat these and check the qualities in bash

## forgot to take off the gz endings
sed -i 's/\.gz//' prokFileList 
## add in the directory for the unzipped fastqs
sed -i 's/^/fionaFastaqsUnz\//' prokFileList

cat $(cat prokFileList) > testprokcat.fastq

## does this work?
for i in $(cat prokFileList)
do
ls -lh $i
done

## run the fastqc on this

fastqc testprokcat.fastq \
-o fastQCoutputs \
--extract

## actually looks okay. lots of trimming, etc. but useable, maybe

## and as far as the euks?


## forgot to take off the gz endings
sed -i 's/\.gz//' eukFileList 
## add in the directory for the unzipped fastqs
sed -i 's/^/fionaFastaqsUnz\//' eukFileList

cat $(cat eukFileList) > testeukcat.fastq

## how many reads are we dealing with?:

grep @FS1 testprokcat.fastq | wc -l ## 112,436, over eleven samples
grep @FS1 testeukcat.fastq | wc -l ## 963,776 reads, over 5 samples. 

## shit, looks like the euks took all the reads, and they are not 
## useable. 

testprokcat.fastq
testeukcat.fastq

fastqc testeukcat.fastq \
-o fastQCoutputs \
--extract

## yeah the shitiness is all in the eukaryotic dataset

## so now what? Seems like here we branch into to two 
## sub-projects. 

## one project is an autopsy on the eukaryotic reads. 
## the other is a diversity analysis of prokaryotic 
## reads. 

## first priority is the 16s data, to get some nice graphs for fiona. 

## at the end of this sub-project we can blast our 18s
## for a match with the pseudogymnoascus. 

## for now, 16s. 


############## 16s ##################

## okay, let's use qiime, see how well that environment 
## is set up for quality filtering/trimming steps

## first, let's split up the fiona directory into 
## euks and proks, not really, just with symlinks

## qiime wants to import the zipped files, so..

## edit our file list again to 

sed -i 's/fionaFastaqsUnz/fionaFastqs/' prokFileList
sed -i 's/\.fastq/\.fastq\.gz/' prokFileList

sed -i 's/fionaFastaqsUnz/fionaFastqs/' eukFileList
sed -i 's/\.fastq/\.fastq\.gz/' eukFileList


mkdir prokFastaQC
mkdir eukFastaQC

for i in $(cat prokFileList); do
    #ln -s $i prokFastaQC${i/fionaFastqs/}
    cp $i prokFastaQC${i/fionaFastqs/}
done

for i in $(cat eukFileList); do
    #ln -s $i eukFastaQC${i/fionaFastqs/}
    cp $i eukFastaQC${i/fionaFastqs/}
done

## so focus on the proks only

conda activate qiime2-2021.11

qiime tools import \
  --type 'SampleData[SequencesWithQuality]' \
  --input-path /home/daniel/Documents/analyses/FionaCave/prokFastLinks/ \
  --input-format CasavaOneEightSingleLanePerSampleDirFmt \
  --output-path fionaRaw16sSeqs.qza

## let's use the dada2 pipeline, for the moment. Because tillmann requested this
## in the future, I'd prefer USEARCH-UNOISE

## ugh. Why use qiime? I'm going to dada2 directly in R, makes more sense. 

## let's see if we can our ASVs out of this this afternoon

## following https://benjjneb.github.io/dada2/tutorial_1_8.html

install.packages('dada2')

BiocManager::install("dada2")

library(dada2)

## our 16s files are here:

path <- "/home/daniel/Documents/analyses/FionaCave/fionaFastaqsUnz/"
sampFiles <- list.files(path)



i <- "Oemik-000724_S138_L001_R1_001.fastq"
png(file=paste(i,"qplot.png",sep='.'))
plotQualityProfile(paste(path,i,sep=''))
dev.off()

system('mkdir dada2QualityPlots16s')

pdf(file="dada2QualityPlots.pdf")
for (i in sampFiles) {
    print(i)
    plotQualityProfile(paste(path,i,sep=''))
}
dev.off()
## that did not work, don't know why. some custom deal with dada2?

## so had to do:

png(file=paste(i,"qplot.png",sep='.'))
plotQualityProfile(paste(path,i,sep=''))
dev.off()

i <- "Oemik-000687_S101_L001_R1_001.fastq"
i <- "Oemik-000688_S102_L001_R1_001.fastq"
i <- "Oemik-000693_S107_L001_R1_001.fastq"
i <- "Oemik-000694_S108_L001_R1_001.fastq"
i <- "Oemik-000698_S112_L001_R1_001.fastq"
i <- "Oemik-000699_S113_L001_R1_001.fastq"
i <- "Oemik-000703_S117_L001_R1_001.fastq"
i <- "Oemik-000704_S118_L001_R1_001.fastq"
i <- "Oemik-000708_S122_L001_R1_001.fastq"
i <- "Oemik-000709_S123_L001_R1_001.fastq"
i <- "Oemik-000713_S127_L001_R1_001.fastq"
i <- "Oemik-000714_S128_L001_R1_001.fastq"
i <- "Oemik-000718_S132_L001_R1_001.fastq"
i <- "Oemik-000719_S133_L001_R1_001.fastq"
i <- "Oemik-000723_S137_L001_R1_001.fastq"
i <- "Oemik-000724_S138_L001_R1_001.fastq"

## ugly. Anyway, most of these bacterial 
## samples start dropping in read quality
## somewhere are 200 bp? this is very 
## low quality data. 

## and we don't have paired reads to 
## make up for this... ugh

## let's try keeping the first 200. 

path <- "/home/daniel/Documents/analyses/FionaCave/fionaFastaqsUnz/"

path <- "/home/daniel/Documents/analyses/FionaCave/fionaFastaqsUnz/"

path <- "/home/daniel/Documents/analyses/FionaCave/prokFastaQC/"

sampFiles <- list.files(path)

help(filterAndTrim)

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE

head(out)



out <- filterAndTrim(  fwd=path,
                        filt='proksFiltTrim',
                        truncLen=200,
                        multithread=TRUE
)

out 

errF <- learnErrors('proksFiltTrim', multithread=TRUE)

pdf(file='errorModelDada2.pdf')
plotErrors(errF, nominalQ=TRUE)
dev.off()

## that is sufficiently fucked up looking.

## I do not know why we are bothering with this data...

## tomorrow, derep, dechimera, tax it up, etc.
