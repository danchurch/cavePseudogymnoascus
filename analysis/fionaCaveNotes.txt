### set up a git repo for this. ###
git remote add origin https://github.com/danchurch/cavePseudogymnoascus.git
git branch -M main
git remote set-url origin git@github.com:danchurch/cavePseudogymnoascus.git
git push -u origin main
###########################################

## see what we can do with the reads from the cave

########## install and play with qiime2 #############

## it is requested that I use qiime2. Great. 

## following:
https://docs.qiime2.org/2021.11/install/native/

## qiime says install in a conda environment. Fine by me, contain the beast. 

wget https://data.qiime2.org/distro/core/qiime2-2021.11-py38-linux-conda.yml

conda env create -n qiime2-2021.11 --file qiime2-2021.11-py38-linux-conda.yml

conda activate qiime2-2021.11 

## great. and to get tab completion, they have a script ready:

source tab-qiime

## not sure if there is some sort of rc for conda environments?
## qiime says put it in bashrc. wait on that. 

#############################

## so, fiona has 16 samples?:

Fiona-1
Fiona-2
Fiona-3
Fiona-4
Fiona-5
Fiona-6
Fiona-7
Fiona-8
FIona-9
Fiona-10
Fiona-11
Fiona-16Euk
Fiona-19Euk
Fiona-20Euk
Fiona-21Euk
Fiona-22Euk

## I see absolutely no evidence anywhere of lab controls, positive 
## or negative. Great, off to a good start. 

## let's get these Fiona samples and remove the others.
## and go fishing for primers...

## do this in pandas

python3
import os
import pandas as pd

readDir = "/home/daniel/Documents/analyses/FionaCave/otherFastqs"

## our read files are:
fastqs = pd.Series(os.listdir(readDir))

## one useful sample info spreadsheet is here
aa = pd.read_excel('/home/daniel/Documents/analyses/FionaCave/metadata/WeigQCchecks/OeMik-Exp009_Proben.xlsx')
## we just need those with fiona in the "Probe Id" column value, 
## and just the "Probe Id" and "Genomics Id" columns: 
bb = aa[["Genomics Id","Probe Id"]]
mask = bb["Probe Id"].str.contains('Fiona').fillna(False)
cc = bb[mask]
## it looks like this missed one sample, where "Fiona" is spelled with a big "I"
cc = cc.append(bb[bb["Probe Id"].str.contains('FIona-9').fillna(False)])
cc.reset_index(inplace=True, drop=True)

## now, we want to go through our fasuqs series, and pull out the matches to our Genomics Id column...

def getFN(GI):
    mask = fastqs.str.contains(GI)
    fn = fastqs[mask].values[0]
    return(fn)

cc['filename'] = cc['Genomics Id'].apply(getFN)

cc.to_csv('fionaFileMap.csv', index=False)

## and for the shell loop, just the filenames:

cc['filename'].to_csv('fionaFileList', header=False, index=False)

## now out to shell, to save just these files:

#mkdir fionaFastqs
ddir=/home/daniel/Documents/analyses/FionaCave/otherFastqs/
fionaFastqs=/home/daniel/Documents/analyses/FionaCave/fionaFastqs/

for x in $(cat fionaFileList)
do
    mv $ddir$x $fionaFastqs
done

## and back into this for for a moment, we want to the illumina indexes
## make sure they aren't floating around somewhere

readDir = "/home/daniel/Documents/analyses/FionaCave/"

os.path.exists(readDir+'fionaFileMap.csv')

aa = pd.read_csv(readDir+'fionaFileMap.csv', index_col='Genomics Id')

## get sample indexes. There are two...why? these are uni-directional, right?
## keep them for the moment, maybe this will make sense later...

indexCSV = "/home/daniel/Documents/analyses/FionaCave/metadata/tillmannPrimer/weigIndexInfo.csv"
os.path.exists(indexCSV)
indexInfo = pd.read_csv(indexCSV, index_col='Sample_Name')
indexInfo = indexInfo[['index', 'index2']]
indexInfo.head()
bb = aa.join(indexInfo, how='left')
bb.reset_index(inplace=True)
bb.columns = ['sampleName','description', 'fileName','index','index2']
## update our fiona file info csv
bb.to_csv('fionaFileMap.csv',  index=False)
bb = pd.read_csv('fionaFileMap.csv')

fionaInfo = pd.read_csv('fionaFileMap.csv')


## great, now we need metadata....


## attach the pool that a sample is from?

## and can we get the forward and reverse 16s and 18s barcodes 
## for these?

## from Tillmann email:

## Prok_16S (~300 bp)
## ilu_515f
## TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTGYCAGCMGCCGCGGTAA
## ilu_806rN
## GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAGGGACTACNVGGGTWTCTAAT

## Euk_18S (~500 bp)
## ilu_GA20F           
## TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTAACTTCGGGAWAAGGATTGGCT
## ilu_RM9R
## GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAGAGAGTCAARCTCAACAGGGTCTT 



## great, so now, let's get some quality graphics for our read sets, etc.

## we want to import our read files into qiime...

## not sure what the illumina pipelines are these days...
## this may be casava1.8?

## an example data set is available from qiime for comparison:

conda activate qiime2-2021.11

exFastq='/home/daniel/Desktop/casava-18-single-end-demultiplexed/L1S8_8_L001_R1_001.fastq'

head $exFastq

## let's check for primers in there...

f16s=TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTGYCAGCMGCCGCGGTAA
r16s=GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAGGGACTACNVGGGTWTCTAAT
f16sRC=TTACCGCGGCMGCTGYCACCTGTCTCTTATACACATCTGACGCTGCCGACGA
r16sRC=ATTAGAWACCCVNGTAGTCCCTGTCTCTTATACACATCTCCGAGCCCACGAGAC

f18s=TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTAACTTCGGGAWAAGGATTGGCT
r18s=GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAGAGAGTCAARCTCAACAGGGTCTT
f18sRC=AGCCAATCCTTWTCCCGAAGTTACCTGTCTCTTATACACATCTGACGCTGCCGACGA
r18sRC=AAGACCCTGTTGAGRTTGACTCTCTGTCTCTTATACACATCTCCGAGCCCACGAGAC

ffq=/home/daniel/Documents/analyses/FionaCave/fionaFastqUnz/

grep -R GAGTCAGCAGC $ffq 
grep -R GAGTCAGCAGC $ffq | wc -l

grep -R $f16s $ffq
grep -R $r16s $ffq
grep -R $f16sRC $ffq
grep -R $r16sRC $ffq

grep -R $f18s $ffq
grep -R $r18s $ffq
grep -R $f18sRC $ffq
grep -R $r18sRC $ffq

## no exact matches. I assume since this is all demultiplexed,
## he pulled out phyx, etc. 

## how are the read depths? and quality

cut -d , -f 2 fionaFileMap.csv

exFQ=/home/daniel/Documents/analyses/FionaCave/fionaFastaqsUnz/Oemik-000687_S101_L001_R1_001.fastq

fastqc fionaFastqs/Oemik-000687_S101_L001_R1_001.fastq.gz  \

fastqc fionaFastqs/*  \
-o fastQCoutputs \
-- extract

## it would be nice to get a total view of the fiona runs,

cat * > allFionaSamples.fastq

fastqc allFionaSamples.fastq \
-o fastQCoutputs \
--extract

## oh jeez. this is some messy data
## the vast majority of the reads are 
## ~150 BP in length (60,000 or so), less than 20000 
## are in the right read length

## might be useful to break this down by primer
## sets...


## so onward with qiime, to bring this in as an "artifact"

conda activate qiime2-2021.11

qiime tools import \
  --type 'SampleData[SequencesWithQuality]' \
  --input-path /home/daniel/Documents/analyses/FionaCave/fionaFastqs \
  --input-format CasavaOneEightSingleLanePerSampleDirFmt \
  --output-path fionaRawSeqs.qza

## and we need some kind of metadata here
## the only metadata I think we have are what pool a sample came from. 

## side note, for github, how do make sure we're not putting on 
## large files, before sending them to github? with find:

find -size +10M -exec du -h {} \;


## okay, for the moment, let's confirm that the Eukaryotic reads are 
## the source of the problem. back in python

## the 16s reads are:

wd = '/home/daniel/Documents/analyses/FionaCave/'

aa = pd.read_csv(wd+'fionaFileMap.csv')

prokDF = aa[~aa['description'].str.contains('Euk')]

prokDF['fileName'].to_csv('prokFileList', header=False, index=False)


eukDF = aa[aa['description'].str.contains('Euk')]

eukDF['fileName'].to_csv('eukFileList', header=False, index=False)

## now, we should be able to cat these and check the qualities in bash

## forgot to take off the gz endings
sed -i 's/\.gz//' prokFileList 
## add in the directory for the unzipped fastqs
sed -i 's/^/fionaFastaqsUnz\//' prokFileList

cat $(cat prokFileList) > testprokcat.fastq

## does this work?
for i in $(cat prokFileList)
do
ls -lh $i
done

## run the fastqc on this

fastqc testprokcat.fastq \
-o fastQCoutputs \
--extract

## actually looks okay. lots of trimming, etc. but useable, maybe

## and as far as the euks?


## forgot to take off the gz endings
sed -i 's/\.gz//' eukFileList 
## add in the directory for the unzipped fastqs
sed -i 's/^/fionaFastaqsUnz\//' eukFileList

cat $(cat eukFileList) > testeukcat.fastq

## how many reads are we dealing with?:

grep @FS1 testprokcat.fastq | wc -l ## 112,436, over eleven samples
grep @FS1 testeukcat.fastq | wc -l ## 963,776 reads, over 5 samples. 

## shit, looks like the euks took all the reads, and they are not 
## useable. 

testprokcat.fastq
testeukcat.fastq

fastqc testeukcat.fastq \
-o fastQCoutputs \
--extract

## yeah the problem is all in the eukaryotic dataset

## so now what? Seems like here we branch into to two 
## sub-projects. 

## one project is an autopsy on the eukaryotic reads. 
## the other is a diversity analysis of prokaryotic 
## reads. 

## first priority is the 16s data, to get some nice graphs for fiona. 

## at the end of this sub-project we can blast our 18s
## for a match with the pseudogymnoascus. 

## for now, 16s. 


############## 16s ##################

## okay, let's use qiime, see how well that environment 
## is set up for quality filtering/trimming steps

## first, let's split up the fiona directory into 
## euks and proks, not really, just with symlinks

## qiime wants to import the zipped files, so..

## edit our file list again to 

sed -i 's/fionaFastaqsUnz/fionaFastqs/' prokFileList
sed -i 's/\.fastq/\.fastq\.gz/' prokFileList

sed -i 's/fionaFastaqsUnz/fionaFastqs/' eukFileList
sed -i 's/\.fastq/\.fastq\.gz/' eukFileList


mkdir prokFastaQC
mkdir eukFastaQC

for i in $(cat prokFileList); do
    #ln -s $i prokFastaQC${i/fionaFastqs/}
    cp $i prokFastaQC${i/fionaFastqs/}
done

for i in $(cat eukFileList); do
    #ln -s $i eukFastaQC${i/fionaFastqs/}
    cp $i eukFastaQC${i/fionaFastqs/}
done

## so focus on the proks only

conda activate qiime2-2021.11

qiime tools import \
  --type 'SampleData[SequencesWithQuality]' \
  --input-path /home/daniel/Documents/analyses/FionaCave/prokFastLinks/ \
  --input-format CasavaOneEightSingleLanePerSampleDirFmt \
  --output-path fionaRaw16sSeqs.qza

## let's use the dada2 pipeline, for the moment. Because tillmann requested this
## in the future, I'd prefer USEARCH-UNOISE

## ugh. Why use qiime? I'm going to dada2 directly in R, makes more sense. 

## let's see if we can our ASVs out of this this afternoon

## following https://benjjneb.github.io/dada2/tutorial_1_8.html

install.packages('dada2')

BiocManager::install("dada2")

library(dada2)
library(phyloseq)

## our 16s files are here:

path <- "/home/daniel/Documents/analyses/FionaCave/fionaFastaqsUnz/"

sampFiles <- list.files(path)

i <- "Oemik-000724_S138_L001_R1_001.fastq"
png(file=paste(i,"qplot.png",sep='.'))
plotQualityProfile(paste(path,i,sep=''))
dev.off()

system('mkdir dada2QualityPlots16s')

pdf(file="dada2QualityPlots.pdf")
for (i in sampFiles) {
    print(i)
    plotQualityProfile(paste(path,i,sep=''))
}
dev.off()
## that did not work, don't know why. some custom deal with dada2?

## so had to do:

png(file=paste(i,"qplot.png",sep='.'))
plotQualityProfile(paste(path,i,sep=''))
dev.off()

i <- "Oemik-000687_S101_L001_R1_001.fastq"
i <- "Oemik-000688_S102_L001_R1_001.fastq"
i <- "Oemik-000693_S107_L001_R1_001.fastq"
i <- "Oemik-000694_S108_L001_R1_001.fastq"
i <- "Oemik-000698_S112_L001_R1_001.fastq"
i <- "Oemik-000699_S113_L001_R1_001.fastq"
i <- "Oemik-000703_S117_L001_R1_001.fastq"
i <- "Oemik-000704_S118_L001_R1_001.fastq"
i <- "Oemik-000708_S122_L001_R1_001.fastq"
i <- "Oemik-000709_S123_L001_R1_001.fastq"
i <- "Oemik-000713_S127_L001_R1_001.fastq"
i <- "Oemik-000714_S128_L001_R1_001.fastq"
i <- "Oemik-000718_S132_L001_R1_001.fastq"
i <- "Oemik-000719_S133_L001_R1_001.fastq"
i <- "Oemik-000723_S137_L001_R1_001.fastq"
i <- "Oemik-000724_S138_L001_R1_001.fastq"

## ugly. Anyway, most of these bacterial 
## samples start dropping in read quality
## somewhere are 200 bp? this is very 
## low quality data. 

## and we don't have paired reads to 
## make up for this... ugh

## let's try keeping the first 200. 
## note: rerunning this after figuring out 
## the primers, so including a left (5') trim of 19 bp

#path <- "/home/daniel/Documents/analyses/FionaCave/fionaFastaqsUnz/"

path <- "/home/daniel/Documents/analyses/FionaCave/prokFastaQC/"

sampFiles <- list.files(path)

help(filterAndTrim)


out <- filterAndTrim(  fwd=path,
                        filt='proksFiltTrim',
                        trimLeft=19,
                        truncLen=200,
                        multithread=TRUE
)

out  ## not a lot lost

## do they look better? run a report

allProksFiltTrimfastq=/home/daniel/Documents/analyses/FionaCave/proksFiltTrimUnz/proksFiltTrim.fastq

fastqc $allProksFiltTrimfastq \
-o fastQCoutputs \
--extract

## generally good. But damn short now.

path <- "/home/daniel/Documents/analyses/FionaCave/proksFiltTrim/"

list.files(path)

## spot checks
i <- "Oemik-000687_S101_L001_R1_001.fastq.gz"

i <- "Oemik-000693_S107_L001_R1_001.fastq.gz"

i <- "Oemik-000713_S127_L001_R1_001.fastq.gz"

plotQualityProfile(paste(path,i,sep=''))

## yeah, looks okay I guess

###### machine learns error #########

errF <- learnErrors('proksFiltTrim', multithread=TRUE)

pdf(file='errorModelDada2.pdf')

plotErrors(errF, nominalQ=TRUE)

dev.off()

## that is sufficiently messed up 

## tomorrow, derep, dechimera, tax it up, etc.

prokErrorModelDada2 <- errF
#save(prokErrorModelDada2, file='prokErrorModelDada2.rda')
#load('prokErrorModelDada2.rda')
errF <- prokErrorModelDada2

## derep

derepProlks <- derepFastq('proksFiltTrim', verbose=TRUE)

#sink(file="derepFirstOutputs.txt")
derepProlks 
#sink()

samplefile.names <- list.files('proksFiltTrim')


getSampName <- function(i) { return(strsplit(i, "_")[[1]][1]) }
sampleNames = sapply(samplefile.names, getSampName)
names(derepProlks) <- sampleNames
dadaProks <- dada(derepProlks, err=errF, multithread=TRUE)

dadaProks

## if we pool the reads, does this change much?


dadaProksPooled <- dada(derepProlks, err=errF, multithread=TRUE, pool=TRUE)

## 

#sink('dadaProks.txt')
dadaProks
#sink()

#sink('dadaProksPooled.txt')
dadaProksPooled
#sink()

## oh jeez, pooling the reads created ~10x as many ASVs...why?
## with this low quality data, I'm going with the more conservative
## method (unpooled). 

## we may have to use an OTU approach anyway, to soften the 
## rough edges of this data

####### make sequence table ########

str(dadaProks)

seqtab <- makeSequenceTable(dadaProks)


seqtab[1:3,1:2]

dim(seqtab)

## just checking:
table(nchar(getSequences(seqtab))) 
## yeah, all exactly 181, as expected

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
## just 2 chimeras removed

dim(seqtab.nochim) ## 1717 asvs

save(seqtab.nochim, file='fionaCaveASVseqTable.csv')

sum(seqtab.nochim)/sum(seqtab) ## 99% retained


sum(seqtab.nochim) ## 101636 reads retained. Out of?

## in shell


sed -i "s/fionaFastqs/fionaFastaqsUnz/" prokFileList

for i in $(cat prokFileList); do
    #ls -l ${i/\.gz}
    grep "@FS" ${i/\.gz} | wc -l
done

## which gives us the following read abundances
7586
31220
7186
7788
6382
8573
7200
8540
9559
11676
6726

aa = c(7586, 31220, 7186, 7788, 6382, 8573, 7200, 8540, 9559, 11676, 6726)

sum(aa) ## 112,436  reads 

## so we lost 

112436-101636

## 10800 reads were lost in this process. Not horrible.



###### assign taxonomy #######

## dada2 includes its own classifier, a naive bayesian classifier like rdp, 
## and has a training data set, all based on silva

## I guess because we are using a slightly older version of dada2, and thus
## following an older tutorial, the silva files available don't quite 
## match the tutorial. let's see if the newest version of silva training 
## set is compatible: 

system("wget https://zenodo.org/record/4587955/files/silva_nr99_v138.1_train_set.fa.gz")

## just use the default bootstrap settings, they probably know better than I do

taxa <- assignTaxonomy(seqtab.nochim, "silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE)

taxTable <- taxa

## adding species requires a separate step:

system("wget https://zenodo.org/record/4587955/files/silva_species_assignment_v138.1.fa.gz")

taxa <- addSpecies(taxa, "silva_species_assignment_v138.1.fa.gz")

taxTable <- taxa

save(taxTable, file="taxTable.rda")

## write just the taxonomic abundances to csv

taxaNoRow <- taxa
rownames(taxaNoRow) <- NULL
write.csv(taxaNoRow, file='taxaNoSequence.csv')

## interesting. Now it's go time with phyloseq, methinks

## need to format our metadata so that it has the right sample names
## so over to python 

import pandas as pd

aa = pd.read_csv("fionaMetadata.csv")

bb = pd.read_csv('fionaFileMap.csv')

## all substrates are sediment, don't need this. 
aa.drop('Substrate', axis=1, inplace=True)
## we want to assign correct sample names to this
aa.rename({'Sample Name':'description'}, axis=1, inplace=True)

bb.set_index('description', inplace=True)

cc = aa.join(bb, how='left', on='description')

dd = cc[['description', 'Pool', 'sampleName']]

dd.set_index("sampleName", inplace=True)

dd.to_csv('fionaMetadata.csv')

############## onto phyloseq ###############

## our metadata is here:

aa <- read.csv('metadata/fionaMetadata.csv', row.names=1)

## subset to prokaryotic samples:
prokMetadata <- aa[row.names(seqtab.nochim),]

prokMetadata 


ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(prokMetadata), 
               tax_table(taxTable))

plot_richness(ps, x="Pool", measures=c("Shannon", "Simpson"), color="When")
## apparently we have no singletons? weird. wonder why?


ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))

ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")

ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray") 

p = plot_ordination(ps.prop, ord.nmds.bray, color="Pool", title="Bray NMDS") 
p$layers <- p$layers[-1]
j=0.05
q = p + geom_jitter(width = j, height=j, size=5, alpha=0.7)
q

s = q + geom_point(size=5, alpha=0.7)
s
## let's revisit that, after we do some breakdowns of the taxonamy

geom_point(size = 1, alpha = 0.3,
               position = position_jitter(width = 0.3)) +

## first start - 



## great. But let's back up and make classic 97% OTUs, as recently requested
## by tillmann

## move all these data objects to somewhere so we don't have to repeat.

fionaASVphyseq <- ps
seqtab.nochim <- asvSeqTabNoChim

save(fionaASVphyseq, file='fionaASVphyseq.rda')

save(asvSeqTabNoChim, file='asvSeqTabNoChim.rda')

## can we use this in qiime?

## back to qiime

conda activate qiime2-2021.11

qiime tools import \
  --type 'SampleData[SequencesWithQuality]' \
  --input-path /home/daniel/Documents/analyses/FionaCave/prokFastaQC \
  --input-format CasavaOneEightSingleLanePerSampleDirFmt \
  --output-path fionaProk16sSeqs_raw.qza

## nope

## start again
## get our metadata how qiime wants it...

## we know we need to trim down to 200 bp, how does qiime want us to do this?

qiime dada2 denoise-single \
    --p-trunc-len 200 \
    --i-demultiplexed-seqs fionaProk16sSeqs_raw.qza \
    --p-trunc-q 32 \
    --output-dir "qiimeDadaOutputs"

## great, that was a fail. For some reason, as implemented here,
## dada2 fails to generate error rates, says too few reads. 
## but it's got lots. Qiime sucks. I always have to do this, 
## dive into the original packages, because they just pile 
## script upon script.

## anyway, if we do come back to the clustering of asvs issue, this looks
## like a good start:

https://groups.google.com/g/vsearch-forum/c/orqWhwGlPPY?pli=1 


## for now its back to the search for primers in the reads. Alfons 
## says they should be in there, my greps don't find them. 
## this means they have some off-target amplification, etc. 

## let's look harder with grep:

proks=/home/daniel/Documents/analyses/FionaCave/prokFastaQC

f16s=TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTGYCAGCMGCCGCGGTAA
r16s=GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAGGGACTACNVGGGTWTCTAAT
f16sRC=TTACCGCGGCMGCTGYCACCTGTCTCTTATACACATCTGACGCTGCCGACGA
r16sRC=ATTAGAWACCCVNGTAGTCCCTGTCTCTTATACACATCTCCGAGCCCACGAGAC

f18s=TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTAACTTCGGGAWAAGGATTGGCT
r18s=GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAGAGAGTCAARCTCAACAGGGTCTT
f18sRC=AGCCAATCCTTWTCCCGAAGTTACCTGTCTCTTATACACATCTGACGCTGCCGACGA
r18sRC=AAGACCCTGTTGAGRTTGACTCTCTGTCTCTTATACACATCTCCGAGCCCACGAGAC

ffq=/home/daniel/Documents/analyses/FionaCave/proksUnzipped/

grep -R GAGTCAGCAGC $ffq 

grep -R GAGTCAGCAGC $ffq | wc -l

grep -R $f16s $ffq
grep -R $r16s $ffq
grep -R $f16sRC $ffq
grep -R $r16sRC $ffq

grep -R $f18s $ffq
grep -R $r18s $ffq
grep -R $f18sRC $ffq
grep -R $r18sRC $ffq

## no complete matches. But our forward matches should be in there
## try some fragments

## f16sFrag=TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTGYCAGCMGCCGCGGTAA

frag=TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTG.CAGC.GCCGCGGTAA ## nope

frag=GTGTATAAGAGACAGGTG ## nope

frag=GTGTATA ## nope

frag=GTGTCAGCCGCCGC

grep -R $frag $ffq

## positions 2, 6, and 5 are listed as highly conserved

## so something like this?:

frag=^.T..CA

grep -R $frag $ffq | wc -l 
## 95705 out of 112436 reads have this

## it's not in our primers:
grep -R $frag <(echo $f16s)

echo ${#f16s} ## primers are 52 bp long. Wtf. 

## hmmm, looks like I may have been searching for too much. try this:

f16s=GTG.CAGC.GCCGCGGTAA
grep -R $f16s $ffq 

grep -R $f16s $ffq | wc -l ## 57731 out of 112436 have this

## jeezus, so our primer sequence is incorrect in over half the 
## the reads. 
## something is really wrong. Oh well. No one is going to publish 
## this thing.

## that's our forward primer. 
echo ${#f16s} ## 19 bp. 

## okay, so given that, let's trim off the first 19 bp. 
## prior to that, remove any reads under 292 bp?
## and also our previous 3' cutoff at original bp200 still
## holds. That will leave us with ~180 bp. Not much. 

## so rerun the native dada2 pipeline with these cutoffs,
## then make pretty graphs for fiona. 

## so jump back to the above sections on dada2...

## and from there, try using adapting vsearch for OTU clustering of our 
## sequences

## what just did was a machine-learning-modified method of dereplication,
## essentially. So we want make an object that vsearch understands as a 
## table of dereplicates with their abundances

## we also need to track which samples of members of these and how many.
## jeezus. 

## this is not a publishable dataset, but this will work for a thesis, I think.

## new plan - use the taxonomy table from the asvs, collapse it by genus. 

## then treat genus as our otus. 

## so back into dada2/phyloseq

library(dada2)
library(phyloseq)
library(ggplot2)
library(Biostrings)

## we need our seq x sample and seq x tax tables:

load('taxTable.rda')


## where is our sample table?  

## ugh, need to rebuild...back in a minute...

## okay, now can we break down the abundances?
## I think we want a rank abundance curve by genus and phylum
## both by individual pool, and overall

## start with the overall, can phyloseq help us?

ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(prokMetadata), 
               tax_table(taxTable))

## let's get the sequence names into refseq format
## and create nicer ASV names
## (this is thanks to)
## https://github.com/joey711/phyloseq/issues/1111
## https://github.com/benjjneb/dada2/issues/613

sequences <- Biostrings::DNAStringSet(taxa_names(ps))
names(sequences) <- taxa_names(ps)
ps <- merge_phyloseq(ps, sequences)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
#save(ps, file='asvPhyloseq.rda')

## that's a relief. 

## write these out to fasta?

refseq(ps)

Biostrings::writeXStringSet(refseq(ps), 'asv.fasta')

## looks great. and especially with the row names
## cleaned up

## now, can we build a tree? out to bash for muscle and phyml...



top20 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:20]

## this is a quick and dirty normalization by total sample abundances

ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.top20)
plot_bar(ps.top20, x="Pool", fill="Phylum")

top30 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:30]
ps.top30 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top30 <- prune_taxa(top30, ps.top30)
plot_bar(ps.top30, x="Pool", fill="Phylum")

## nice, but too many lines. Can we merge OTUs counts?


#aa <- ps

aa <- merge_samples(ps, "Pool")
top30 <- names(sort(taxa_sums(aa), decreasing=TRUE))[1:30]
aa.top30 <- transform_sample_counts(aa, function(OTU) OTU/sum(OTU))
aa.top30 <- prune_taxa(top30, aa.top30)

#png('top30GeneraByPhylum_stacked.png', width=1200, height=1000)
plot_bar(aa.top30, fill="Phylum")
#dev.off()

rownames(bb) <- NULL

bb <- tax_table(aa)[top30,]


aa

## I think technically we should be doing something deseq to stabilize these,
## but oh well.


## how do we merge to genus level?

bb = tax_glom(ps, "Genus", NArm=TRUE)
bbNA = tax_glom(ps, "Genus", NArm=FALSE)

bb

bbNA

bb ## this takes us to 129 taxa (genera), from 1717 ASVs

cc = tax_table(bb); row.names(cc) <- NULL
write.csv(cc, file='generaMerge.csv')
ccNA = tax_table(bbNA); row.names(ccNA) <- NULL
write.csv(ccNA, file='generaMergeNA.csv')

## when we keep all sequences, even those not identified to 
## to genus, we have 374 otus. When we throw out the unidentified 
## genera, we have 129 otus. So our tip_glom, based on sequence
## similarity, should probably land somewhere in between those 
## two:

## the default
bb = tip_glom(ps, h=0.2)
aa <- merge_samples(bb, "Pool")

top30 <- names(sort(taxa_sums(bb), decreasing=TRUE))[1:30]
aa.top30 <- transform_sample_counts(bb, function(OTU) OTU/sum(OTU))
aa.top30 <- prune_taxa(top30, aa.top30)


plot_bar(aa.top30, fill="Phylum")

plot_bar(aa.top30, fill="Genus")

## I think we need a 16s tree...
## we need a fasta file. So out to muscle and phyml for this?

## muscle 

muscle -in asv.fasta -out fionaASVs.phy -phyi

## great. Now feed this to phyml? 

## just because of this paper, just using defaults

phyml -i fionaASVs.phy -b 100

## then what?

## we use this to construct a unifrac based ordination
## of the communities, which will be more robust than 
## the quick and dirty NMS above

## what do we need to show for fiona?


## 1. csv of genera and their abundaces for pool, ranked
## 2. bargraph 30 most common genera and their relative abundances in each pool colored by genus

## 3. csv of genus level OTUs and their abundances for pool, ranked
## 4. bargraph 30 most common OTUs and their relative abundances in each pool colored by genus

## 5. same bargraph, colored by phylum? 

## 6. bc and unifrac NMS ordinations and permanovas 

## 1. csv of genera and their abundaces for pool, ranked

load('asvPhyloseq.rda')

## collapse genus
#bb = tax_glom(ps, "Genus", NArm=TRUE)
bb = tax_glom(ps, "Genus", NArm=FALSE)
## for one pool
aa <- merge_samples(bb, "Pool")

## break out one pool:

GP.chl = prune_samples(sampleSums(GP.chl)>=20, GP.chl)

write.csv(t(otu_table(ps)), file="asv_table.csv")

otu_table(aa)[1,]

cc <- prune_samples(sample_data(ps)$Pool == "K1", ps)


## 2. bargraph 30 most common genera and their relative abundances in each pool colored by genus

#png(file="top20GeneraByPhyla.csv")
#aa <- merge_samples(bb, "Pool")

aa = tax_glom(ps, "Genus", NArm=FALSE)
bb <- merge_samples(aa, "Pool")

top20 <- names(sort(taxa_sums(aa), decreasing=TRUE))[1:20]
aa.top20 <- transform_sample_counts(aa, function(OTU) OTU/sum(OTU))
aa.top20 <- prune_taxa(top20, aa.top20)

#png(file="top20GeneraByPhyla.csv")


#png(file="graphicsTables/top20GeneraByPhylaBySample.csv")
plot_bar(aa.top20, fill="Phylum")
#dev.off()


#plot_bar(aa.top20, fill="Genus")

## let's try to get rank abundances for each pool of the common 
## genera

nGen <- 20

lev <- "Kingdom"
lev <- "Phylum"
lev <- "Class"
lev <- "Order"
lev <- "Family"

lev <- "Genus"
lev <- "Species"
categ <- "K1"
categ <- "M2"
categ <- "M3"
categ <- "M4"

## subset to a category
aa <- subset_samples(ps, Pool == categ)
## get rid of zeroes
#otu_table(aa) == 0
zeroes <- !colSums(otu_table(aa)) == 0
bb <- prune_taxa(zeroes, aa)
## sort and clean
cc <- colSums((otu_table(bb)))
dd <- sort(cc, decreasing=TRUE)
## prune down common OTUs
ee <- dd[1:nGen]
## relabel with genus:
ff <- ee
#names(ff) <- tax_table(aa)[names(ee),"Genus"]
names(ff) <- tax_table(aa)[names(ee),lev]
## plot
## bottom, left, top, right
par(mai =  c(2.5, 1, 0.5, 0.5))
barplot(ff, 
        las=2, 
        main=categ, 
        cex.main=3, 
        cex.axis=1,
        cex.lab=2,
)


#png(file='top20GeneraByPhyla.png', width=1600, height=1200)
#png(file='top20GeneraByGenus.png', width=1600, height=1200)
par(mfrow=c(2,2))

dev.off()

#### NMS ####
ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")

png(file='superRawNMS.png', width=600, height=600)
p = plot_ordination(ps.prop, ord.nmds.bray, color="Pool", title="Bray NMDS") 
p$layers <- p$layers[-1]
#p
j=0.05
q = p + geom_jitter(width = j, height=j, size=5, alpha=0.7)
q
dev.off()

#################################################################
## side note, I seem to have lost my qc report for the untrimmed 
## prokaryotic samples, rebuild:
allProks=/home/daniel/Documents/analyses/FionaCave/proksUnzipped/
cat $allProks* > allProkSamples.fastq
fastqc allProkSamples.fastq -o fastQCoutputs
#################################################################


ps.genus = tax_glom(ps, "Genus", NArm=FALSE)

ps.genus.pool <- merge_samples(ps.genus, "Pool")

ps
ps.genus
ps.genus.pool

sample_data(ps.genus.pool)

otu_table(ps.genus.pool)

tax_table(ps.genus.pool)

## we need rank abundances by pool, with taxonomy. How to do this...


dim(tax_table(ps.genus.pool))

## essentially we are melding two tables, one of taxonomy 
## and one of abundances in each table. 

## with each we'll have the asv name as the key.

#aa <- tax_table(ps)
aa <- tax_table(ps.genus.pool)

row.names(aa)

## abundances in each sample...how to do this...
bb <- t(otu_table(ps.genus.pool))

all(row.names(aa) == row.names(bb))

all(row.names(aa) == row.names(bb))

## i think the merge function should do this for us.


cc <- merge(aa,bb, by="row.names")


all(is.na(cc$Species))

new_df <- subset(cc, select = -c(Species))

new_df <- subset(cc, select = -Species)

## seems right. Out to Tillmann?

write.csv(cc, file='fiona16sTaxReadAbundencesByPoolTable.csv')

## what else? we need to get the black boxes out of the stacked bargraph

## so just recolor them by genus?

#############################

## recent request by tillmann - 

## redo the by-pool abundances, broken apart by sample

## forgot that they want to do everything by genus, not asv...

ps.genus = tax_glom(ps, "Genus", NArm=FALSE)

## this OTU table, what does it look like?

t(otu_table(ps.genus))

t(otu_table(ps))

## first, to make exactly the table tillmann requested,
## I think this is the pipeline we need:

ps.genus = tax_glom(ps, "Genus", NArm=FALSE)

aa <- tax_table(ps.genus)
bb <- t(otu_table(ps.genus))
cc <- merge(aa,bb, by="row.names")
colnames(cc)[colnames(cc) == "Row.names"] = "repASV"

## just check, any issues there?
unique(cc$Genus)
dd <- cc$Genus[!is.na(cc$Genus)]
duplicated(dd)
## looks okay

write.csv(cc, file= "./graphicsTables/genusBySampleWithRawReadAbundances.csv", row.names=FALSE)


