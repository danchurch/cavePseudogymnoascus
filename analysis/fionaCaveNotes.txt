### set up a git repo for this. ###
git remote add origin https://github.com/danchurch/cavePseudogymnoascus.git
git branch -M main
git remote set-url origin git@github.com:danchurch/cavePseudogymnoascus.git
git push -u origin main
###########################################

## see what we can do with the reads from the cave

########## install and play with qiime2 #############

## it is requested that I use qiime2. Great. 

## following:
https://docs.qiime2.org/2021.11/install/native/

## qiime says install in a conda environment. Fine by me, contain the beast. 

wget https://data.qiime2.org/distro/core/qiime2-2021.11-py38-linux-conda.yml

conda env create -n qiime2-2021.11 --file qiime2-2021.11-py38-linux-conda.yml

conda activate qiime2-2021.11 

## great. and to get tab completion, they have a script ready:

source tab-qiime

## not sure if there is some sort of rc for conda environments?
## qiime says put it in bashrc. wait on that. 

#############################

## so, fiona has 16 samples?:

Fiona-1
Fiona-2
Fiona-3
Fiona-4
Fiona-5
Fiona-6
Fiona-7
Fiona-8
FIona-9
Fiona-10
Fiona-11
Fiona-16Euk
Fiona-19Euk
Fiona-20Euk
Fiona-21Euk
Fiona-22Euk

## let's get these and remove the others.
## and go fishing for primers...

## do this in pandas

python3
import os
import pandas as pd

readDir = "/home/daniel/Documents/analyses/FionaCave/otherFastqs"

## our read files are:
fastqs = pd.Series(os.listdir(readDir))

## one useful sample info spreadsheet is here
aa = pd.read_excel('/home/daniel/Documents/analyses/FionaCave/metadata/WeigQCchecks/OeMik-Exp009_Proben.xlsx')
## we just need those with fiona in the "Probe Id" column value, 
## and just the "Probe Id" and "Genomics Id" columns: 
bb = aa[["Genomics Id","Probe Id"]]
mask = bb["Probe Id"].str.contains('Fiona').fillna(False)
cc = bb[mask]
## it looks like this missed one sample, where "Fiona" is spelled with a big "I"
cc = cc.append(bb[bb["Probe Id"].str.contains('FIona-9').fillna(False)])
cc.reset_index(inplace=True, drop=True)

## now, we want to go through our fasuqs series, and pull out the matches to our Genomics Id column...

def getFN(GI):
    mask = fastqs.str.contains(GI)
    fn = fastqs[mask].values[0]
    return(fn)

cc['filename'] = cc['Genomics Id'].apply(getFN)

cc.to_csv('fionaFileMap.csv', index=False)

## and for the shell loop, just the filenames:

cc['filename'].to_csv('fionaFileList', header=False, index=False)

## now out to shell, to save just these files:

#mkdir fionaFastqs
ddir=/home/daniel/Documents/analyses/FionaCave/otherFastqs/
fionaFastqs=/home/daniel/Documents/analyses/FionaCave/fionaFastqs/

for x in $(cat fionaFileList)
do
    mv $ddir$x $fionaFastqs
done

## and back into this for for a moment, we want to the illumina indexes
## make sure they aren't floating around somewhere

readDir = "/home/daniel/Documents/analyses/FionaCave/"

os.path.exists(readDir+'fionaFileMap.csv')

aa = pd.read_csv(readDir+'fionaFileMap.csv', index_col='Genomics Id')

## get sample indexes. There are two...why? these are uni-directional, right?
## keep them for the moment, maybe this will make sense later...

indexCSV = "/home/daniel/Documents/analyses/FionaCave/metadata/tillmannPrimer/weigIndexInfo.csv"
os.path.exists(indexCSV)
indexInfo = pd.read_csv(indexCSV, index_col='Sample_Name')
indexInfo = indexInfo[['index', 'index2']]
indexInfo.head()
bb = aa.join(indexInfo, how='left')
bb.reset_index(inplace=True)
bb.columns = ['sampleName','description', 'fileName','index','index2']
## update our fiona file info csv
bb.to_csv('fionaFileMap.csv',  index=False)
bb = pd.read_csv('fionaFileMap.csv')

fionaInfo = pd.read_csv('fionaFileMap.csv')


## great, now we need metadata....


## attach the pool that a sample is from?

## and can we get the forward and reverse 16s and 18s barcodes 
## for these?

## from Tillmann email:

## Prok_16S (~300 bp)
## ilu_515f
## TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTGYCAGCMGCCGCGGTAA
## ilu_806rN
## GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAGGGACTACNVGGGTWTCTAAT

## Euk_18S (~500 bp)
## ilu_GA20F           
## TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTAACTTCGGGAWAAGGATTGGCT
## ilu_RM9R
## GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAGAGAGTCAARCTCAACAGGGTCTT 



## great, so now, let's get some quality graphics for our read sets, etc.

## we want to import our read files into qiime...

## not sure what the illumina pipelines are these days...
## this may be casava1.8?

## an example data set is available from qiime for comparison:

conda activate qiime2-2021.11

exFastq='/home/daniel/Desktop/casava-18-single-end-demultiplexed/L1S8_8_L001_R1_001.fastq'

head $exFastq

## let's check for primers in there...

f16s=TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTGYCAGCMGCCGCGGTAA
r16s=GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAGGGACTACNVGGGTWTCTAAT
f16sRC=TTACCGCGGCMGCTGYCACCTGTCTCTTATACACATCTGACGCTGCCGACGA
r16sRC=ATTAGAWACCCVNGTAGTCCCTGTCTCTTATACACATCTCCGAGCCCACGAGAC

f18s=TCGTCGGCAGCGTCAGATGTGTATAAGAGACAGGTAACTTCGGGAWAAGGATTGGCT
r18s=GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAGAGAGTCAARCTCAACAGGGTCTT
f18sRC=AGCCAATCCTTWTCCCGAAGTTACCTGTCTCTTATACACATCTGACGCTGCCGACGA
r18sRC=AAGACCCTGTTGAGRTTGACTCTCTGTCTCTTATACACATCTCCGAGCCCACGAGAC

ffq=/home/daniel/Documents/analyses/FionaCave/fionaFastqUnz/

grep -R GAGTCAGCAGC $ffq 
grep -R GAGTCAGCAGC $ffq | wc -l

grep -R $f16s $ffq
grep -R $r16s $ffq
grep -R $f16sRC $ffq
grep -R $r16sRC $ffq

grep -R $f18s $ffq
grep -R $r18s $ffq
grep -R $f18sRC $ffq
grep -R $r18sRC $ffq

## no exact matches. I assume since this is all demultiplexed,
## he pulled out phyx, etc. 

## how are the read depths? and quality

cut -d , -f 2 fionaFileMap.csv

exFQ=/home/daniel/Documents/analyses/FionaCave/fionaFastaqsUnz/Oemik-000687_S101_L001_R1_001.fastq

fastqc fionaFastqs/Oemik-000687_S101_L001_R1_001.fastq.gz  \

fastqc fionaFastqs/*  \
-o fastQCoutputs \
-- extract

## it would be nice to get a total view of the fiona runs,

cat * > allFionaSamples.fastq

fastqc allFionaSamples.fastq \
-o fastQCoutputs \
--extract

## oh jeez. this is some messy data
## the vast majority of the reads are 
## ~150 BP in length (60,000 or so), less than 20000 
## are in the right read length

## might be useful to break this down by primer
## sets...


## so onward with qiime, to bring this in as an "artifact"

conda activate qiime2-2021.11

qiime tools import \
  --type 'SampleData[SequencesWithQuality]' \
  --input-path /home/daniel/Documents/analyses/FionaCave/fionaFastqs \
  --input-format CasavaOneEightSingleLanePerSampleDirFmt \
  --output-path fionaRawSeqs.qza

## and we need some kind of metadata here
## the only metadata I think we have are what pool a sample came from. 

## side note, for github, how do make sure we're not putting on 
## large files, before sending them to github? with find:

find -size +10M -exec du -h {} \;


## okay, for the moment, let's confirm that the Eukaryotic reads are 
## the source of the problem. back in python

## the 16s reads are:

wd = '/home/daniel/Documents/analyses/FionaCave/'

aa = pd.read_csv(wd+'fionaFileMap.csv')

prokDF = aa[~aa['description'].str.contains('Euk')]

prokDF['fileName'].to_csv('prokFileList', header=False, index=False)


eukDF = aa[aa['description'].str.contains('Euk')]

eukDF['fileName'].to_csv('eukFileList', header=False, index=False)

## now, we should be able to cat these and check the qualities in bash

## forgot to take off the gz endings
sed -i 's/\.gz//' prokFileList 
## add in the directory for the unzipped fastqs
sed -i 's/^/fionaFastaqsUnz\//' prokFileList

cat $(cat prokFileList) > testprokcat.fastq

## does this work?
for i in $(cat prokFileList)
do
ls -lh $i
done

## run the fastqc on this

fastqc testprokcat.fastq \
-o fastQCoutputs \
--extract

## actually looks okay. lots of trimming, etc. but useable, maybe

## and as far as the euks?


## forgot to take off the gz endings
sed -i 's/\.gz//' eukFileList 
## add in the directory for the unzipped fastqs
sed -i 's/^/fionaFastaqsUnz\//' eukFileList

cat $(cat eukFileList) > testeukcat.fastq

## how many reads are we dealing with?:

grep @FS1 testprokcat.fastq | wc -l ## 112,436, over eleven samples
grep @FS1 testeukcat.fastq | wc -l ## 963,776 reads, over 5 samples. 

## shit, looks like the euks took all the reads, and they are not 
## useable. 

testprokcat.fastq
testeukcat.fastq

fastqc testeukcat.fastq \
-o fastQCoutputs \
--extract

## yeah the shitiness is all in the eukaryotic dataset

## so now what? Seems like here we branch into to two 
## sub-projects. 

## one project is an autopsy on the eukaryotic reads. 
## the other is a diversity analysis of prokaryotic 
## reads. 

## first priority is the 16s data, to get some nice graphs for fiona. 

## at the end of this sub-project we can blast our 18s
## for a match with the pseudogymnoascus. 

## for now, 16s. 


############## 16s ##################

## okay, let's use qiime, see how well that environment 
## is set up for quality filtering/trimming steps

## first, let's split up the fiona directory into 
## euks and proks, not really, just with symlinks

## qiime wants to import the zipped files, so..

## edit our file list again to 

sed -i 's/fionaFastaqsUnz/fionaFastqs/' prokFileList
sed -i 's/\.fastq/\.fastq\.gz/' prokFileList

sed -i 's/fionaFastaqsUnz/fionaFastqs/' eukFileList
sed -i 's/\.fastq/\.fastq\.gz/' eukFileList


mkdir prokFastaQC
mkdir eukFastaQC

for i in $(cat prokFileList); do
    #ln -s $i prokFastaQC${i/fionaFastqs/}
    cp $i prokFastaQC${i/fionaFastqs/}
done

for i in $(cat eukFileList); do
    #ln -s $i eukFastaQC${i/fionaFastqs/}
    cp $i eukFastaQC${i/fionaFastqs/}
done

## so focus on the proks only

conda activate qiime2-2021.11

qiime tools import \
  --type 'SampleData[SequencesWithQuality]' \
  --input-path /home/daniel/Documents/analyses/FionaCave/prokFastLinks/ \
  --input-format CasavaOneEightSingleLanePerSampleDirFmt \
  --output-path fionaRaw16sSeqs.qza

## let's use the dada2 pipeline, for the moment. Because tillmann requested this
## in the future, I'd prefer USEARCH-UNOISE

## ugh. Why use qiime? I'm going to dada2 directly in R, makes more sense. 

## let's see if we can our ASVs out of this this afternoon

## following https://benjjneb.github.io/dada2/tutorial_1_8.html

install.packages('dada2')

BiocManager::install("dada2")

library(dada2)
library(phyloseq)

## our 16s files are here:

path <- "/home/daniel/Documents/analyses/FionaCave/fionaFastaqsUnz/"

sampFiles <- list.files(path)

i <- "Oemik-000724_S138_L001_R1_001.fastq"
png(file=paste(i,"qplot.png",sep='.'))
plotQualityProfile(paste(path,i,sep=''))
dev.off()

system('mkdir dada2QualityPlots16s')

pdf(file="dada2QualityPlots.pdf")
for (i in sampFiles) {
    print(i)
    plotQualityProfile(paste(path,i,sep=''))
}
dev.off()
## that did not work, don't know why. some custom deal with dada2?

## so had to do:

png(file=paste(i,"qplot.png",sep='.'))
plotQualityProfile(paste(path,i,sep=''))
dev.off()

i <- "Oemik-000687_S101_L001_R1_001.fastq"
i <- "Oemik-000688_S102_L001_R1_001.fastq"
i <- "Oemik-000693_S107_L001_R1_001.fastq"
i <- "Oemik-000694_S108_L001_R1_001.fastq"
i <- "Oemik-000698_S112_L001_R1_001.fastq"
i <- "Oemik-000699_S113_L001_R1_001.fastq"
i <- "Oemik-000703_S117_L001_R1_001.fastq"
i <- "Oemik-000704_S118_L001_R1_001.fastq"
i <- "Oemik-000708_S122_L001_R1_001.fastq"
i <- "Oemik-000709_S123_L001_R1_001.fastq"
i <- "Oemik-000713_S127_L001_R1_001.fastq"
i <- "Oemik-000714_S128_L001_R1_001.fastq"
i <- "Oemik-000718_S132_L001_R1_001.fastq"
i <- "Oemik-000719_S133_L001_R1_001.fastq"
i <- "Oemik-000723_S137_L001_R1_001.fastq"
i <- "Oemik-000724_S138_L001_R1_001.fastq"

## ugly. Anyway, most of these bacterial 
## samples start dropping in read quality
## somewhere are 200 bp? this is very 
## low quality data. 

## and we don't have paired reads to 
## make up for this... ugh

## let's try keeping the first 200. 


#path <- "/home/daniel/Documents/analyses/FionaCave/fionaFastaqsUnz/"

path <- "/home/daniel/Documents/analyses/FionaCave/prokFastaQC/"

sampFiles <- list.files(path)

help(filterAndTrim)



out <- filterAndTrim(  fwd=path,
                        filt='proksFiltTrim',
                        truncLen=200,
                        multithread=TRUE
)

out 

## just checking, how do the libraries look now?:

path <- "/home/daniel/Documents/analyses/FionaCave/proksFiltTrim/"

list.files(path)

## spot checks
i <- "Oemik-000687_S101_L001_R1_001.fastq.gz"
i <- "Oemik-000693_S107_L001_R1_001.fastq.gz"
i <- "Oemik-000713_S127_L001_R1_001.fastq.gz"

plotQualityProfile(paste(path,i,sep=''))

## yeah, looks okay

###### machine learns error #########

errF <- learnErrors('proksFiltTrim', multithread=TRUE)

pdf(file='errorModelDada2.pdf')

plotErrors(errF, nominalQ=TRUE)

dev.off()

## that is sufficiently fucked up looking.

## I do not know why we are bothering with this data...

## tomorrow, derep, dechimera, tax it up, etc.

prokErrorModelDada2 <- errF
save(prokErrorModelDada2, file='prokErrorModelDada2.rda')
errF <- prokErrorModelDada2

## derep

derepProlks <- derepFastq('proksFiltTrim', verbose=TRUE)

#sink(file="derepFirstOutputs.txt")
derepProlks 
#sink()

samplefile.names <- list.files('proksFiltTrim')


getSampName <- function(i) { return(strsplit(i, "_")[[1]][1]) }

sampleNames = sapply(samplefile.names, getSampName)

names(derepProlks) <- sampleNames

dadaProks <- dada(derepProlks, err=errF, multithread=TRUE)

dadaProks

## if we pool the reads, does this change much?


dadaProksPooled <- dada(derepProlks, err=errF, multithread=TRUE, pool=TRUE)

## 

#sink('dadaProks.txt')
dadaProks
#sink()

#sink('dadaProksPooled.txt')
dadaProksPooled
#sink()

## oh jeez, pooling the reads created ~10x as many ASVs...why?
## with this low quality data, I'm going with the more conservative
## method (unpooled). 

## we may have to use an OTU approach anyway, to soften the 
## rough edges of this shitty data

####### make sequence table ########

str(dadaProks)

seqtab <- makeSequenceTable(dadaProks)

seqtab[1:3,1:2]

dim(seqtab)

## just checking:
table(nchar(getSequences(seqtab))) 
## yeah, all exactly 200, as expected

seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
## 83 chimeras removed

dim(seqtab.nochim) ## 1236 asvs

dim(seqtab) ## 1319 asvs

sum(seqtab.nochim)/sum(seqtab)

## 92% asvs retained. 

sum(seqtab.nochim) ## 85201 reads retained. Out of?

## in shell


sed -i "s/fionaFastqs/fionaFastaqsUnz/" prokFileList

for i in $(cat prokFileList); do
    #ls -l ${i/\.gz}
    grep "@FS" ${i/\.gz} | wc -l
done

## which gives us the following read abundances
7586
31220
7186
7788
6382
8573
7200
8540
9559
11676
6726

aa = c(7586, 31220, 7186, 7788, 6382, 8573, 7200, 8540, 9559, 11676, 6726)

sum(aa) ## 112,436  reads 

## so we lost 

112436-85201

## 27,235 reads were lost in this process. Not horrible.

###### assign taxonomy #######

## dada2 includes its own classifier, a naive bayesian classifier like rdp, 
## and has a training data set, all based on silva

## I guess because we are using a slightly older version of dada2, and thus
## following an older tutorial, the silva files available don't quite 
## match the tutorial. let's see if the newest version of silva training 
## set is compatible: 

system("wget https://zenodo.org/record/4587955/files/silva_nr99_v138.1_train_set.fa.gz")

## just use the default bootstrap settings, they probably know better than I do

taxa <- assignTaxonomy(seqtab.nochim, "silva_nr99_v138.1_train_set.fa.gz", multithread=TRUE)

taxTable <- taxa
save(taxTable, file="taxTable.rda")

## adding species requires a separate step:

system("wget https://zenodo.org/record/4587955/files/silva_species_assignment_v138.1.fa.gz")

taxa <- addSpecies(taxa, "silva_species_assignment_v138.1.fa.gz")

## write to csv
taxaNoRow <- taxa
rownames(taxaNoRow) <- NULL
write.csv(taxaNoRow, file='taxaNoRow.csv')

## interesting. Now it's go time with phyloseq, methinks

## need to format our metadata so that it has the right sample names
## so over to python 

import pandas as pd

aa = pd.read_csv("fionaMetadata.csv")

bb = pd.read_csv('fionaFileMap.csv')

## all substrates are sediment, don't need this. 
aa.drop('Substrate', axis=1, inplace=True)
## we want to assign correct sample names to this
aa.rename({'Sample Name':'description'}, axis=1, inplace=True)

bb.set_index('description', inplace=True)

cc = aa.join(bb, how='left', on='description')

dd = cc[['description', 'Pool', 'sampleName']]

dd.set_index("sampleName", inplace=True)

dd.to_csv('fionaMetadata.csv')

############## onto phyloseq ###############

## our metadata is here:

aa <- read.csv('metadata/fionaMetadata.csv', row.names=1)

## subset to prokaryotic samples:

prokMetadata <- aa[row.names(seqtab.nochim),]

prokMetadata 

ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(prokMetadata), 
               tax_table(taxTable))

plot_richness(ps, x="Pool", measures=c("Shannon", "Simpson"), color="When")

ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")

plot_ordination(ps.prop, ord.nmds.bray, color="Pool", title="Bray NMDS")
